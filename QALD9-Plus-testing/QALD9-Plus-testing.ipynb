{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating DBpedia queries to Wikidata with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary libraries\n",
    "import re\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset for asking the LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset saved to 'llm_input_dataset.json'.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from 100_complete_entries.json\n",
    "with open(\"100_complete_entries.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create input dataset for the LLM\n",
    "llm_inputs = []\n",
    "\n",
    "for entry in data:\n",
    "    # Extract required fields\n",
    "    question = entry.get(\"question\", \"\")  # Natural language question\n",
    "    dbpedia_query = entry.get(\"dbpedia_query\", \"\")  # SPARQL query for DBpedia\n",
    "    \n",
    "    # Extract entities and relations (ER2) in Wikidata\n",
    "    er2 = [\n",
    "        {\n",
    "            \"dbpedia_id\": er[\"dbpedia_id\"],\n",
    "            \"wikidata_ids\": er[\"wikidata_ids\"]\n",
    "        }\n",
    "        for er in entry.get(\"mapped_entities_relations\", {}).get(\"entities_relations\", [])\n",
    "        if er[\"wikidata_ids\"]  # Only include non-empty Wikidata mappings\n",
    "    ]\n",
    "    \n",
    "    # Skip if there are no valid Wikidata mappings\n",
    "    if not er2:\n",
    "        continue\n",
    "\n",
    "    # Construct the input for the LLM\n",
    "    llm_input = {\n",
    "        \"context\": {\n",
    "            \"natural_language_question\": question,\n",
    "            \"sparql_query_kg1\": dbpedia_query,\n",
    "            \"kg1_name\": \"DBpedia\",\n",
    "            \"kg2_name\": \"Wikidata\",\n",
    "            \"er2\": er2,\n",
    "            \"instruction\": \"Given the information above, produce a SPARQL query for KG2.\"\n",
    "        }\n",
    "    }\n",
    "    llm_inputs.append(llm_input)\n",
    "\n",
    "# Save the processed dataset to a new JSON file\n",
    "with open(\"llm_input_dataset.json\", \"w\") as file:\n",
    "    json.dump(llm_inputs, file, indent=4)\n",
    "\n",
    "print(f\"Processed dataset saved to 'llm_input_dataset.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First test with 10 sample queries\n",
    "Using meta-llama-3.1-8b-instruct for the first test of translating the queries from DBpedia to Wikidata. The connection works and the model is able to translate the queries. The output is quite large, because the model also explains what as been done. Should the prompt be changed to get a more concise output, only the complete query?\n",
    "\n",
    "**meta-llama-3.1-8b-instruct is the smallest available model at Academic Cloud https://chat-ai.academiccloud.de/chat, with 8billion parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated SPARQL queries for 10 questions saved to 'translated_llm_output_10_queries.json'.\n"
     ]
    }
   ],
   "source": [
    "# Set up the LLM API connection\n",
    "api_key = '###'\n",
    "base_url = \"https://chat-ai.academiccloud.de/v1\"\n",
    "model = \"meta-llama-3.1-8b-instruct\"  # Replace with the appropriate model\n",
    "\n",
    "# Start OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "# Load the input dataset\n",
    "with open(\"llm_input_dataset.json\", \"r\") as file:\n",
    "    llm_input_data = json.load(file)\n",
    "\n",
    "# Initialize the list to store the responses\n",
    "translated_dataset = []\n",
    "\n",
    "# Limit to the first 10 queries\n",
    "limited_input_data = llm_input_data[:10]\n",
    "\n",
    "# Query the LLM for each entry in the limited dataset\n",
    "for entry in limited_input_data:\n",
    "    context = entry[\"context\"]\n",
    "    \n",
    "    # Create the prompt for the LLM\n",
    "    prompt = (\n",
    "        f\"Context:\\n\"\n",
    "        f\"Natural Language Question: {context['natural_language_question']}\\n\"\n",
    "        f\"SPARQL Query for KG1 ({context['kg1_name']}):\\n\"\n",
    "        f\"{context['sparql_query_kg1']}\\n\"\n",
    "        f\"Knowledge Graph 1 Name: {context['kg1_name']}\\n\"\n",
    "        f\"Knowledge Graph 2 Name: {context['kg2_name']}\\n\"\n",
    "        f\"Entity and Relation Mapping (ER2):\\n{json.dumps(context['er2'], indent=2)}\\n\"\n",
    "        f\"Instruction: {context['instruction']}\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Query the LLM\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # Extract the generated SPARQL query from the response\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Append the response to the translated dataset\n",
    "        translated_entry = {\n",
    "            \"context\": context,\n",
    "            \"sparql_query_kg2\": response_text  # Add the translated SPARQL query\n",
    "        }\n",
    "        translated_dataset.append(translated_entry)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying LLM for question ID {context.get('natural_language_question', 'unknown')}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save the translated dataset to a new JSON file\n",
    "with open(\"translated_llm_output_10_queries_meta-llama-3.1-8b.json\", \"w\") as file:\n",
    "    json.dump(translated_dataset, file, indent=4)\n",
    "\n",
    "print(\"Translated SPARQL queries for 10 questions saved to 'translated_llm_output_10_queries_meta-llama-3.1-8b.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for Mistral-Large-Instruct \n",
    "The model includes 123billion parameters and is the largest available model at Academic Cloud https://chat-ai.academiccloud.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated SPARQL queries for 10 questions saved to 'translated_llm_output_10_queries_mistral.json'.\n"
     ]
    }
   ],
   "source": [
    "# Set up the LLM API connection\n",
    "api_key = '###'\n",
    "base_url = \"https://chat-ai.academiccloud.de/v1\"\n",
    "model = \"mistral-large-instruct\"  # Updated to use the new model\n",
    "\n",
    "# Start OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "# Load the input dataset\n",
    "with open(\"llm_input_dataset.json\", \"r\") as file:\n",
    "    llm_input_data = json.load(file)\n",
    "\n",
    "# Initialize the list to store the responses\n",
    "translated_dataset = []\n",
    "\n",
    "# Limit to the first 10 queries\n",
    "limited_input_data = llm_input_data[:10]\n",
    "\n",
    "# Query the LLM for each entry in the limited dataset\n",
    "for entry in limited_input_data:\n",
    "    context = entry[\"context\"]\n",
    "    \n",
    "    # Create the prompt for the LLM\n",
    "    prompt = (\n",
    "        f\"Context:\\n\"\n",
    "        f\"Natural Language Question: {context['natural_language_question']}\\n\"\n",
    "        f\"SPARQL Query for KG1 ({context['kg1_name']}):\\n\"\n",
    "        f\"{context['sparql_query_kg1']}\\n\"\n",
    "        f\"Knowledge Graph 1 Name: {context['kg1_name']}\\n\"\n",
    "        f\"Knowledge Graph 2 Name: {context['kg2_name']}\\n\"\n",
    "        f\"Entity and Relation Mapping (ER2):\\n{json.dumps(context['er2'], indent=2)}\\n\"\n",
    "        f\"Instruction: {context['instruction']}\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Query the LLM\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # Extract the generated SPARQL query from the response\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Append the response to the translated dataset\n",
    "        translated_entry = {\n",
    "            \"context\": context,\n",
    "            \"sparql_query_kg2\": response_text  # Add the translated SPARQL query\n",
    "        }\n",
    "        translated_dataset.append(translated_entry)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying LLM for question ID {context.get('natural_language_question', 'unknown')}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save the translated dataset to a new JSON file\n",
    "with open(\"translated_llm_output_10_queries_mistral.json\", \"w\") as file:\n",
    "    json.dump(translated_dataset, file, indent=4)\n",
    "\n",
    "print(\"Translated SPARQL queries for 10 questions saved to 'translated_llm_output_10_queries_mistral.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing meta-llama-3.1-8b-instruct with 100 queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated SPARQL queries for all questions saved to 'translated_llm_output_meta-llama-3.1-8b.json'.\n"
     ]
    }
   ],
   "source": [
    "# Set up the LLM API connection\n",
    "api_key = '###'\n",
    "base_url = \"https://chat-ai.academiccloud.de/v1\"\n",
    "model = \"meta-llama-3.1-8b-instruct\"  # Replace with the appropriate model\n",
    "\n",
    "# Start OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "# Load the input dataset\n",
    "with open(\"llm_input_dataset.json\", \"r\") as file:\n",
    "    llm_input_data = json.load(file)\n",
    "\n",
    "# Initialize the list to store the responses\n",
    "translated_dataset = []\n",
    "\n",
    "# Query the LLM for each entry in the dataset\n",
    "for entry in llm_input_data:\n",
    "    context = entry[\"context\"]\n",
    "    \n",
    "    # Create the prompt for the LLM\n",
    "    prompt = (\n",
    "        f\"Context:\\n\"\n",
    "        f\"Natural Language Question: {context['natural_language_question']}\\n\"\n",
    "        f\"SPARQL Query for KG1 ({context['kg1_name']}):\\n\"\n",
    "        f\"{context['sparql_query_kg1']}\\n\"\n",
    "        f\"Knowledge Graph 1 Name: {context['kg1_name']}\\n\"\n",
    "        f\"Knowledge Graph 2 Name: {context['kg2_name']}\\n\"\n",
    "        f\"Entity and Relation Mapping (ER2):\\n{json.dumps(context['er2'], indent=2)}\\n\"\n",
    "        f\"Instruction: {context['instruction']}\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Query the LLM\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # Extract the generated SPARQL query from the response\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Append the response to the translated dataset\n",
    "        translated_entry = {\n",
    "            \"context\": context,\n",
    "            \"sparql_query_kg2\": response_text  # Add the translated SPARQL query\n",
    "        }\n",
    "        translated_dataset.append(translated_entry)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying LLM for question ID {context.get('natural_language_question', 'unknown')}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save the translated dataset to a new JSON file\n",
    "with open(\"translated_llm_output_meta-llama-3.1-8b.json\", \"w\") as file:\n",
    "    json.dump(translated_dataset, file, indent=4)\n",
    "\n",
    "print(\"Translated SPARQL queries for all questions saved to 'translated_llm_output_meta-llama-3.1-8b.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Mistral-Large-instruct with 100 queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated SPARQL queries for all questions saved to 'translated_llm_output_mistral.json'.\n"
     ]
    }
   ],
   "source": [
    "# Set up the LLM API connection\n",
    "api_key = '###'\n",
    "base_url = \"https://chat-ai.academiccloud.de/v1\"\n",
    "model = \"mistral-large-instruct\"\n",
    "\n",
    "# Start OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "# Load the input dataset\n",
    "with open(\"llm_input_dataset.json\", \"r\") as file:\n",
    "    llm_input_data = json.load(file)\n",
    "\n",
    "# Initialize the list to store the responses\n",
    "translated_dataset = []\n",
    "\n",
    "# Query the LLM for each entry in the dataset\n",
    "for entry in llm_input_data:\n",
    "    context = entry[\"context\"]\n",
    "    \n",
    "    # Create the prompt for the LLM\n",
    "    prompt = (\n",
    "        f\"Context:\\n\"\n",
    "        f\"Natural Language Question: {context['natural_language_question']}\\n\"\n",
    "        f\"SPARQL Query for KG1 ({context['kg1_name']}):\\n\"\n",
    "        f\"{context['sparql_query_kg1']}\\n\"\n",
    "        f\"Knowledge Graph 1 Name: {context['kg1_name']}\\n\"\n",
    "        f\"Knowledge Graph 2 Name: {context['kg2_name']}\\n\"\n",
    "        f\"Entity and Relation Mapping (ER2):\\n{json.dumps(context['er2'], indent=2)}\\n\"\n",
    "        f\"Instruction: {context['instruction']}\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Query the LLM\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # Extract the generated SPARQL query from the response\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Append the response to the translated dataset\n",
    "        translated_entry = {\n",
    "            \"context\": context,\n",
    "            \"sparql_query_kg2\": response_text  # Add the translated SPARQL query\n",
    "        }\n",
    "        translated_dataset.append(translated_entry)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying LLM for question ID {context.get('natural_language_question', 'unknown')}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save the translated dataset to a new JSON file\n",
    "with open(\"translated_llm_output_mistral.json\", \"w\") as file:\n",
    "    json.dump(translated_dataset, file, indent=4)\n",
    "\n",
    "print(\"Translated SPARQL queries for all questions saved to 'translated_llm_output_mistral.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the results\n",
    "For now it can be seen that the extraction process for the larger model (mistral-large-instruct) is working better than for the smaller model (meta-llama-3.1-8b-instruct). The output of the smaller model is not that structured making it harder to extract the queries. On the first sight it also seems that the queries from the larger model are working better.\n",
    "\n",
    "\n",
    "Extracting the sprql queries from LLM output for **mistral-large-instruct**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted SPARQL queries for the first 10 entries have been saved to mistral_wiki_trans_sparql_queries.json.\n"
     ]
    }
   ],
   "source": [
    "# Load the input JSON file\n",
    "file_path = \"translated_llm_output_mistral.json\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Function to extract SPARQL query from the `sparql_query_kg2` field\n",
    "def extract_sparql_query(entry):\n",
    "    raw_query = entry.get(\"sparql_query_kg2\", \"\")\n",
    "    # Extract content between SPARQL code blocks (```sparql and ``` or similar markers)\n",
    "    match = re.search(r\"```sparql\\n(.*?)\\n```\", raw_query, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).replace(\"\\n\", \" \").strip()  # Remove newline characters and trim whitespace\n",
    "    return None\n",
    "\n",
    "# Extract SPARQL queries\n",
    "queries_with_context = []\n",
    "for entry in data:\n",
    "    sparql_query = extract_sparql_query(entry)\n",
    "    if sparql_query:\n",
    "        queries_with_context.append({\n",
    "            \"natural_language_question\": entry[\"context\"][\"natural_language_question\"],\n",
    "            \"sparql_query_kg2\": sparql_query\n",
    "        })\n",
    "\n",
    "# Save the extracted queries to a new JSON file\n",
    "output_file = \"mistral_wiki_trans_sparql_queries.json\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(queries_with_context, file, indent=4)\n",
    "\n",
    "print(f\"Extracted SPARQL queries for the first 10 entries have been saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the sprql queries from LLM output for **lama-3.1-8b-instruct**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working best so far for lama-3.1-8b-instruct but still need adjustments to extract the queries more reliably. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and extracted data has been saved to lama_wiki_trans_sparql_queries.json.\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON file\n",
    "input_file_path = 'translated_llm_output_meta-llama-3.1-8b.json'\n",
    "output_file_path = 'lama_wiki_trans_sparql_queries.json'\n",
    "\n",
    "with open(input_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Function to extract clean SPARQL queries\n",
    "def extract_sparql_query(sparql_raw):\n",
    "    # Extract query inside code block or with SPARQL syntax markers\n",
    "    sparql_match = re.search(r\"```sparql\\n(.*?)\\n```|PREFIX.*?WHERE\\s*\\{.*?\\}\", sparql_raw, re.DOTALL)\n",
    "    if sparql_match:\n",
    "        # Clean query by removing extra whitespace and comments\n",
    "        query = sparql_match.group(0)\n",
    "        query = re.sub(r\"#.*\", \"\", query)  # Remove comments\n",
    "        query = re.sub(r\"\\s+\", \" \", query).strip()  # Remove extra spaces/newlines\n",
    "        return query\n",
    "    return None\n",
    "\n",
    "# Process the data to extract required fields\n",
    "result = []\n",
    "for entry in data:\n",
    "    context = entry.get('context', {})\n",
    "    natural_language_question = context.get('natural_language_question', None)\n",
    "    sparql_query_raw = entry.get('sparql_query_kg2', '')\n",
    "\n",
    "    # Extract and clean SPARQL query\n",
    "    sparql_query = extract_sparql_query(sparql_query_raw)\n",
    "\n",
    "    if natural_language_question and sparql_query:\n",
    "        result.append({\n",
    "            \"natural_language_question\": natural_language_question,\n",
    "            \"sparql_query\": sparql_query\n",
    "        })\n",
    "\n",
    "# Save the cleaned and extracted data to a new JSON file\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    json.dump(result, output_file, indent=4)\n",
    "\n",
    "print(f\"Cleaned and extracted data has been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not working that well but could maybe help later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted SPARQL queries saved to <_io.TextIOWrapper name='lama2_wiki_trans_sparql_queries.json' mode='w' encoding='cp1252'>.\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON file\n",
    "file_path = \"translated_llm_output_meta-llama-3.1-8b.json\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Function to extract SPARQL query from the sparql_query_kg2 field\n",
    "def extract_sparql_query(sparql_text):\n",
    "    try:\n",
    "        # Find the start of the SPARQL query\n",
    "        start_idx = sparql_text.find(\"```sparql\")\n",
    "        if start_idx == -1:\n",
    "            return None  # No SPARQL query found\n",
    "        \n",
    "        # Find the end of the SPARQL query\n",
    "        end_idx = sparql_text.find(\"```\", start_idx + 8)\n",
    "        if end_idx == -1:\n",
    "            end_idx = len(sparql_text)\n",
    "        \n",
    "        # Extract the SPARQL query\n",
    "        sparql_query = sparql_text[start_idx + 8:end_idx].strip()\n",
    "        return sparql_query\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting SPARQL query: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract SPARQL queries for all entries\n",
    "extracted_queries = []\n",
    "for entry in data:\n",
    "    sparql_text = entry.get(\"sparql_query_kg2\", \"\")\n",
    "    sparql_query = extract_sparql_query(sparql_text)\n",
    "    if sparql_query:\n",
    "        extracted_queries.append(sparql_query)\n",
    "\n",
    "# Save the extracted queries to a new JSON file\n",
    "output_file = \"lama2_wiki_trans_sparql_queries.json\"\n",
    "with open(output_file, \"w\") as output_file:\n",
    "    json.dump(extracted_queries, output_file, indent=4)\n",
    "\n",
    "print(f\"Extracted SPARQL queries saved to {output_file}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
