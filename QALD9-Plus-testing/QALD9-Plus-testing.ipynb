{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating DBpedia queries to Wikidata with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary libraries\n",
    "import requests\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset for asking the LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset saved to 'llm_input_dataset.json'.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from 100_complete_entries.json\n",
    "with open(\"100_complete_entries.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create input dataset for the LLM\n",
    "llm_inputs = []\n",
    "\n",
    "for entry in data:\n",
    "    # Extract required fields\n",
    "    question = entry.get(\"question\", \"\")  # Natural language question\n",
    "    dbpedia_query = entry.get(\"dbpedia_query\", \"\")  # SPARQL query for DBpedia\n",
    "    \n",
    "    # Extract entities and relations (ER2) in Wikidata\n",
    "    er2 = [\n",
    "        {\n",
    "            \"dbpedia_id\": er[\"dbpedia_id\"],\n",
    "            \"wikidata_ids\": er[\"wikidata_ids\"]\n",
    "        }\n",
    "        for er in entry.get(\"mapped_entities_relations\", {}).get(\"entities_relations\", [])\n",
    "        if er[\"wikidata_ids\"]  # Only include non-empty Wikidata mappings\n",
    "    ]\n",
    "    \n",
    "    # Skip if there are no valid Wikidata mappings\n",
    "    if not er2:\n",
    "        continue\n",
    "\n",
    "    # Construct the input for the LLM\n",
    "    llm_input = {\n",
    "        \"context\": {\n",
    "            \"natural_language_question\": question,\n",
    "            \"sparql_query_kg1\": dbpedia_query,\n",
    "            \"kg1_name\": \"DBpedia\",\n",
    "            \"kg2_name\": \"Wikidata\",\n",
    "            \"er2\": er2,\n",
    "            \"instruction\": \"Given the information above, produce a SPARQL query for KG2.\"\n",
    "        }\n",
    "    }\n",
    "    llm_inputs.append(llm_input)\n",
    "\n",
    "# Save the processed dataset to a new JSON file\n",
    "with open(\"llm_input_dataset.json\", \"w\") as file:\n",
    "    json.dump(llm_inputs, file, indent=4)\n",
    "\n",
    "print(f\"Processed dataset saved to 'llm_input_dataset.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First test with 10 sample queries\n",
    "Using meta-llama-3.1-8b-instruct for the first test of translating the queries from DBpedia to Wikidata. The connection works and the model is able to translate the queries. The output is quite large, because the model also explains what as been done. Should the prompt be changed to get a more concise output, only the complete query?\n",
    "\n",
    "**meta-llama-3.1-8b-instruct is the smallest available model at Academic Cloud https://chat-ai.academiccloud.de/chat, with 8billion parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated SPARQL queries for 10 questions saved to 'translated_llm_output_10_queries.json'.\n"
     ]
    }
   ],
   "source": [
    "# Set up the LLM API connection\n",
    "api_key = '0e18102475c8460f5469f8c98b0a9390'\n",
    "base_url = \"https://chat-ai.academiccloud.de/v1\"\n",
    "model = \"meta-llama-3.1-8b-instruct\"  # Replace with the appropriate model\n",
    "\n",
    "# Start OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "# Load the input dataset\n",
    "with open(\"llm_input_dataset.json\", \"r\") as file:\n",
    "    llm_input_data = json.load(file)\n",
    "\n",
    "# Initialize the list to store the responses\n",
    "translated_dataset = []\n",
    "\n",
    "# Limit to the first 10 queries\n",
    "limited_input_data = llm_input_data[:10]\n",
    "\n",
    "# Query the LLM for each entry in the limited dataset\n",
    "for entry in limited_input_data:\n",
    "    context = entry[\"context\"]\n",
    "    \n",
    "    # Create the prompt for the LLM\n",
    "    prompt = (\n",
    "        f\"Context:\\n\"\n",
    "        f\"Natural Language Question: {context['natural_language_question']}\\n\"\n",
    "        f\"SPARQL Query for KG1 ({context['kg1_name']}):\\n\"\n",
    "        f\"{context['sparql_query_kg1']}\\n\"\n",
    "        f\"Knowledge Graph 1 Name: {context['kg1_name']}\\n\"\n",
    "        f\"Knowledge Graph 2 Name: {context['kg2_name']}\\n\"\n",
    "        f\"Entity and Relation Mapping (ER2):\\n{json.dumps(context['er2'], indent=2)}\\n\"\n",
    "        f\"Instruction: {context['instruction']}\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Query the LLM\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # Extract the generated SPARQL query from the response\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Append the response to the translated dataset\n",
    "        translated_entry = {\n",
    "            \"context\": context,\n",
    "            \"sparql_query_kg2\": response_text  # Add the translated SPARQL query\n",
    "        }\n",
    "        translated_dataset.append(translated_entry)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying LLM for question ID {context.get('natural_language_question', 'unknown')}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save the translated dataset to a new JSON file\n",
    "with open(\"translated_llm_output_10_queries_meta-llama-3.1-8b.json\", \"w\") as file:\n",
    "    json.dump(translated_dataset, file, indent=4)\n",
    "\n",
    "print(\"Translated SPARQL queries for 10 questions saved to 'translated_llm_output_10_queries_meta-llama-3.1-8b.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for Mistral-Large-Instruct \n",
    "The model includes 123billion parameters and is the largest available model at Academic Cloud https://chat-ai.academiccloud.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated SPARQL queries for 10 questions saved to 'translated_llm_output_10_queries_mistral.json'.\n"
     ]
    }
   ],
   "source": [
    "# Set up the LLM API connection\n",
    "api_key = '0e18102475c8460f5469f8c98b0a9390'\n",
    "base_url = \"https://chat-ai.academiccloud.de/v1\"\n",
    "model = \"mistral-large-instruct\"  # Updated to use the new model\n",
    "\n",
    "# Start OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "# Load the input dataset\n",
    "with open(\"llm_input_dataset.json\", \"r\") as file:\n",
    "    llm_input_data = json.load(file)\n",
    "\n",
    "# Initialize the list to store the responses\n",
    "translated_dataset = []\n",
    "\n",
    "# Limit to the first 10 queries\n",
    "limited_input_data = llm_input_data[:10]\n",
    "\n",
    "# Query the LLM for each entry in the limited dataset\n",
    "for entry in limited_input_data:\n",
    "    context = entry[\"context\"]\n",
    "    \n",
    "    # Create the prompt for the LLM\n",
    "    prompt = (\n",
    "        f\"Context:\\n\"\n",
    "        f\"Natural Language Question: {context['natural_language_question']}\\n\"\n",
    "        f\"SPARQL Query for KG1 ({context['kg1_name']}):\\n\"\n",
    "        f\"{context['sparql_query_kg1']}\\n\"\n",
    "        f\"Knowledge Graph 1 Name: {context['kg1_name']}\\n\"\n",
    "        f\"Knowledge Graph 2 Name: {context['kg2_name']}\\n\"\n",
    "        f\"Entity and Relation Mapping (ER2):\\n{json.dumps(context['er2'], indent=2)}\\n\"\n",
    "        f\"Instruction: {context['instruction']}\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Query the LLM\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # Extract the generated SPARQL query from the response\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Append the response to the translated dataset\n",
    "        translated_entry = {\n",
    "            \"context\": context,\n",
    "            \"sparql_query_kg2\": response_text  # Add the translated SPARQL query\n",
    "        }\n",
    "        translated_dataset.append(translated_entry)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying LLM for question ID {context.get('natural_language_question', 'unknown')}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save the translated dataset to a new JSON file\n",
    "with open(\"translated_llm_output_10_queries_mistral.json\", \"w\") as file:\n",
    "    json.dump(translated_dataset, file, indent=4)\n",
    "\n",
    "print(\"Translated SPARQL queries for 10 questions saved to 'translated_llm_output_10_queries_mistral.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing meta-llama-3.1-8b-instruct with 100 queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated SPARQL queries for all questions saved to 'translated_llm_output_meta-llama-3.1-8b.json'.\n"
     ]
    }
   ],
   "source": [
    "# Set up the LLM API connection\n",
    "api_key = '0e18102475c8460f5469f8c98b0a9390'\n",
    "base_url = \"https://chat-ai.academiccloud.de/v1\"\n",
    "model = \"meta-llama-3.1-8b-instruct\"  # Replace with the appropriate model\n",
    "\n",
    "# Start OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "# Load the input dataset\n",
    "with open(\"llm_input_dataset.json\", \"r\") as file:\n",
    "    llm_input_data = json.load(file)\n",
    "\n",
    "# Initialize the list to store the responses\n",
    "translated_dataset = []\n",
    "\n",
    "# Query the LLM for each entry in the dataset\n",
    "for entry in llm_input_data:\n",
    "    context = entry[\"context\"]\n",
    "    \n",
    "    # Create the prompt for the LLM\n",
    "    prompt = (\n",
    "        f\"Context:\\n\"\n",
    "        f\"Natural Language Question: {context['natural_language_question']}\\n\"\n",
    "        f\"SPARQL Query for KG1 ({context['kg1_name']}):\\n\"\n",
    "        f\"{context['sparql_query_kg1']}\\n\"\n",
    "        f\"Knowledge Graph 1 Name: {context['kg1_name']}\\n\"\n",
    "        f\"Knowledge Graph 2 Name: {context['kg2_name']}\\n\"\n",
    "        f\"Entity and Relation Mapping (ER2):\\n{json.dumps(context['er2'], indent=2)}\\n\"\n",
    "        f\"Instruction: {context['instruction']}\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Query the LLM\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # Extract the generated SPARQL query from the response\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Append the response to the translated dataset\n",
    "        translated_entry = {\n",
    "            \"context\": context,\n",
    "            \"sparql_query_kg2\": response_text  # Add the translated SPARQL query\n",
    "        }\n",
    "        translated_dataset.append(translated_entry)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying LLM for question ID {context.get('natural_language_question', 'unknown')}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save the translated dataset to a new JSON file\n",
    "with open(\"translated_llm_output_meta-llama-3.1-8b.json\", \"w\") as file:\n",
    "    json.dump(translated_dataset, file, indent=4)\n",
    "\n",
    "print(\"Translated SPARQL queries for all questions saved to 'translated_llm_output_meta-llama-3.1-8b.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Mistral-Large-instruct with 100 queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error querying LLM for question ID What is the alma mater of the chancellor of Germany Angela Merkel?: Error code: 429 - {'message': 'API rate limit exceeded', 'request_id': 'a1b81e0222a68175698df5355412f323'}\n",
      "Error querying LLM for question ID Who created Goofy?: Error code: 429 - {'message': 'API rate limit exceeded', 'request_id': '9ff17b1bd4cf323ea091f82ed0e8a659'}\n",
      "Translated SPARQL queries for all questions saved to 'translated_llm_output_mistral.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up the LLM API connection\n",
    "api_key = '0e18102475c8460f5469f8c98b0a9390' \n",
    "base_url = \"https://chat-ai.academiccloud.de/v1\"\n",
    "model = \"mistral-large-instruct\"\n",
    "\n",
    "# Start OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "# Load the input dataset\n",
    "with open(\"llm_input_dataset.json\", \"r\") as file:\n",
    "    llm_input_data = json.load(file)\n",
    "\n",
    "# Initialize the list to store the responses\n",
    "translated_dataset = []\n",
    "\n",
    "# Query the LLM for each entry in the dataset\n",
    "for entry in llm_input_data:\n",
    "    context = entry[\"context\"]\n",
    "    \n",
    "    # Create the prompt for the LLM\n",
    "    prompt = (\n",
    "        f\"Context:\\n\"\n",
    "        f\"Natural Language Question: {context['natural_language_question']}\\n\"\n",
    "        f\"SPARQL Query for KG1 ({context['kg1_name']}):\\n\"\n",
    "        f\"{context['sparql_query_kg1']}\\n\"\n",
    "        f\"Knowledge Graph 1 Name: {context['kg1_name']}\\n\"\n",
    "        f\"Knowledge Graph 2 Name: {context['kg2_name']}\\n\"\n",
    "        f\"Entity and Relation Mapping (ER2):\\n{json.dumps(context['er2'], indent=2)}\\n\"\n",
    "        f\"Instruction: {context['instruction']}\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Query the LLM\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # Extract the generated SPARQL query from the response\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Append the response to the translated dataset\n",
    "        translated_entry = {\n",
    "            \"context\": context,\n",
    "            \"sparql_query_kg2\": response_text  # Add the translated SPARQL query\n",
    "        }\n",
    "        translated_dataset.append(translated_entry)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying LLM for question ID {context.get('natural_language_question', 'unknown')}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save the translated dataset to a new JSON file\n",
    "with open(\"translated_llm_output_mistral.json\", \"w\") as file:\n",
    "    json.dump(translated_dataset, file, indent=4)\n",
    "\n",
    "print(\"Translated SPARQL queries for all questions saved to 'translated_llm_output_mistral.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
