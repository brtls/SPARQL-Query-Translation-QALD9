{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating Wikidata queries to DBpedia with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary libraries\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset for asking the LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset saved to 'llm_input_dataset_wikidata.json'.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from 100_complete_entries.json\n",
    "with open(\"../../../data/100_complete_entries_wikidata.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create input dataset for the LLM\n",
    "llm_inputs = []\n",
    "\n",
    "for entry in data:\n",
    "    # Extract required fields\n",
    "    question = entry.get(\"question\", \"\")  # Natural language question\n",
    "    wikidata_query = entry.get(\"wikidata_query\", \"\")  # SPARQL query for wikidata\n",
    "    \n",
    "    # Extract entities and relations (ER2) in dbpedia\n",
    "    er2 = [\n",
    "        {\n",
    "            \"dbpedia_id\": er[\"dbpedia_id\"],\n",
    "            \"wikidata_ids\": er[\"wikidata_ids\"]\n",
    "        }\n",
    "        for er in entry.get(\"mapped_entities_relations\", {}).get(\"entities_relations\", [])\n",
    "        if er[\"dbpedia_id\"]  # Only include non-empty dbpedia mappings\n",
    "    ]\n",
    "    \n",
    "    # Skip if there are no valid Wikidata mappings\n",
    "    if not er2:\n",
    "        continue\n",
    "\n",
    "    # Construct the input for the LLM\n",
    "    llm_input = {\n",
    "        \"context\": {\n",
    "            \"natural_language_question\": question,\n",
    "            \"sparql_query_kg1\": wikidata_query,\n",
    "            \"kg1_name\": \"Wikidata\",\n",
    "            \"kg2_name\": \"DBpedia\",\n",
    "            \"er2\": er2,\n",
    "            \"instruction\": \"Given the information above, produce a SPARQL query for KG2. In your answer please hightlight the final, complete SPARQL query within the tags '<sparql>' and '</sparql>'.\"\n",
    "        }\n",
    "    }\n",
    "    llm_inputs.append(llm_input)\n",
    "\n",
    "# Save the processed dataset to a new JSON file\n",
    "with open(\"llm_input_dataset_wikidata.json\", \"w\") as file:\n",
    "    json.dump(llm_inputs, file, indent=4)\n",
    "\n",
    "print(f\"Processed dataset saved to 'llm_input_dataset_wikidata.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing meta-llama-3.1-8b-instruct with 100 queries\n",
    "The model includes 8billion parameters and is one of the samallest available models at Academic Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM API connection\n",
    "api_key = '###'\n",
    "base_url = \"https://chat-ai.academiccloud.de/v1\"\n",
    "model = \"meta-llama-3.1-8b-instruct\"  # Replace with the appropriate model\n",
    "\n",
    "# Start OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "# Load the input dataset\n",
    "with open(\"llm_input_dataset_wikidata.json\", \"r\") as file:\n",
    "    llm_input_data = json.load(file)\n",
    "\n",
    "# Initialize the list to store the responses\n",
    "translated_dataset = []\n",
    "\n",
    "# Query the LLM for each entry in the dataset\n",
    "for entry in llm_input_data:\n",
    "    context = entry[\"context\"]\n",
    "    \n",
    "    # Create the prompt for the LLM\n",
    "    prompt = (\n",
    "        f\"Context:\\n\"\n",
    "        f\"Natural Language Question: {context['natural_language_question']}\\n\"\n",
    "        f\"SPARQL Query for KG1 ({context['kg1_name']}):\\n\"\n",
    "        f\"{context['sparql_query_kg1']}\\n\"\n",
    "        f\"Knowledge Graph 1 Name: {context['kg1_name']}\\n\"\n",
    "        f\"Knowledge Graph 2 Name: {context['kg2_name']}\\n\"\n",
    "        f\"Entity and Relation Mapping (ER2):\\n{json.dumps(context['er2'], indent=2)}\\n\"\n",
    "        f\"Instruction: {context['instruction']}\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Query the LLM\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # Extract the generated SPARQL query from the response\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Append the response to the translated dataset\n",
    "        translated_entry = {\n",
    "            \"context\": context,\n",
    "            \"sparql_query_kg2\": response_text  # Add the translated SPARQL query\n",
    "        }\n",
    "        translated_dataset.append(translated_entry)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying LLM for question ID {context.get('natural_language_question', 'unknown')}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save the translated dataset to a new JSON file\n",
    "with open(\"translated_llm_output_meta-llama-3.1-8b_wikidata.json\", \"w\") as file:\n",
    "    json.dump(translated_dataset, file, indent=4)\n",
    "\n",
    "print(\"Translated SPARQL queries for all questions saved to 'translated_llm_output_meta-llama-3.1-8b_wikidata.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Mistral-Large-instruct with 100 queries\n",
    "The model includes 123billion parameters and is the largest available model at Academic Cloud https://chat-ai.academiccloud.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM API connection\n",
    "api_key = '###'\n",
    "base_url = \"https://chat-ai.academiccloud.de/v1\"\n",
    "model = \"mistral-large-instruct\"\n",
    "\n",
    "# Start OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "# Load the input dataset\n",
    "with open(\"llm_input_dataset_wikidata.json\", \"r\") as file:\n",
    "    llm_input_data = json.load(file)\n",
    "\n",
    "# Initialize the list to store the responses\n",
    "translated_dataset = []\n",
    "\n",
    "# Query the LLM for each entry in the dataset\n",
    "for entry in llm_input_data:\n",
    "    context = entry[\"context\"]\n",
    "    \n",
    "    # Create the prompt for the LLM\n",
    "    prompt = (\n",
    "        f\"Context:\\n\"\n",
    "        f\"Natural Language Question: {context['natural_language_question']}\\n\"\n",
    "        f\"SPARQL Query for KG1 ({context['kg1_name']}):\\n\"\n",
    "        f\"{context['sparql_query_kg1']}\\n\"\n",
    "        f\"Knowledge Graph 1 Name: {context['kg1_name']}\\n\"\n",
    "        f\"Knowledge Graph 2 Name: {context['kg2_name']}\\n\"\n",
    "        f\"Entity and Relation Mapping (ER2):\\n{json.dumps(context['er2'], indent=2)}\\n\"\n",
    "        f\"Instruction: {context['instruction']}\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Query the LLM\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # Extract the generated SPARQL query from the response\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Append the response to the translated dataset\n",
    "        translated_entry = {\n",
    "            \"context\": context,\n",
    "            \"sparql_query_kg2\": response_text  # Add the translated SPARQL query\n",
    "        }\n",
    "        translated_dataset.append(translated_entry)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying LLM for question ID {context.get('natural_language_question', 'unknown')}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save the translated dataset to a new JSON file\n",
    "with open(\"translated_llm_output_mistral_wikidata.json\", \"w\") as file:\n",
    "    json.dump(translated_dataset, file, indent=4)\n",
    "\n",
    "print(\"Translated SPARQL queries for all questions saved to 'translated_llm_output_mistral_wikidata.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the results\n",
    "Extracting the SPARQL queries from LLM output for **mistral-large-instruct**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input JSON file\n",
    "file_path = \"translated_llm_output_mistral.json\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Function to extract SPARQL query from the `sparql_query_kg2` field and remove comments\n",
    "def extract_sparql_query(entry):\n",
    "    raw_query = entry.get(\"sparql_query_kg2\", \"\")\n",
    "    # Remove comments starting with `#` and clean up spaces\n",
    "    cleaned_query = re.sub(r\"#.*\", \"\", raw_query)  # Remove everything after `#` on each line\n",
    "    # Extract content between SPARQL code blocks (```sparql and ``` or similar markers)\n",
    "    match = re.search(r\"```sparql\\n(.*?)\\n```\", cleaned_query, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).replace(\"\\n\", \" \").strip()  # Remove newline characters and trim whitespace\n",
    "    return None\n",
    "\n",
    "# Extract SPARQL queries\n",
    "queries_with_context = []\n",
    "for entry in data:\n",
    "    sparql_query = extract_sparql_query(entry)\n",
    "    if sparql_query:\n",
    "        queries_with_context.append({\n",
    "            \"natural_language_question\": entry[\"context\"][\"natural_language_question\"],\n",
    "            \"sparql_query_kg2\": sparql_query\n",
    "        })\n",
    "\n",
    "# Save the extracted queries to a new JSON file\n",
    "output_file = \"mistral_wiki_trans_sparql_queries.json\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(queries_with_context, file, indent=4)\n",
    "\n",
    "print(f\"Cleaned and extracted data has been saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the SPARQL queries from LLM output for **lama-3.1-8b-instruct**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = 'translated_llm_output_meta-llama-3.1-8b.json'\n",
    "output_file_path = 'lama_wiki_trans_sparql_queries.json'\n",
    "\n",
    "# Load the JSON file\n",
    "with open(input_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Function to extract and clean SPARQL queries\n",
    "def extract_and_clean_sparql_query(sparql_raw):\n",
    "    try:\n",
    "        # Match SPARQL code blocks or queries starting with SPARQL keywords\n",
    "        sparql_match = re.search(r\"```sparql\\s*(.*?)\\s*```|PREFIX.*?WHERE\\s*\\{.*?\\}\", sparql_raw, re.DOTALL)\n",
    "        if sparql_match:\n",
    "            # Extract the matched SPARQL query\n",
    "            query = sparql_match.group(1) or sparql_match.group(0)\n",
    "            # Clean query by removing comments and excessive whitespace\n",
    "            query = re.sub(r\"#.*\", \"\", query)  # Remove comments\n",
    "            query = re.sub(r\"\\s+\", \" \", query).strip()  # Remove excessive whitespace and newlines\n",
    "            # Remove surrounding ```sparql and ```\n",
    "            query = query.replace(\"```sparql\", \"\").replace(\"```\", \"\").strip()\n",
    "            return query\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning SPARQL query: {e}\")\n",
    "    return None\n",
    "\n",
    "# Process all entries and keep natural language question with the query\n",
    "result = []\n",
    "for entry in data:  # Process all entries\n",
    "    context = entry.get('context', {})\n",
    "    natural_language_question = context.get('natural_language_question', None)\n",
    "    sparql_query_raw = entry.get('sparql_query_kg2', '')\n",
    "\n",
    "    # Extract and clean the SPARQL query\n",
    "    sparql_query = extract_and_clean_sparql_query(sparql_query_raw)\n",
    "\n",
    "    if natural_language_question and sparql_query:\n",
    "        result.append({\n",
    "            \"natural_language_question\": natural_language_question,\n",
    "            \"sparql_query\": sparql_query\n",
    "        })\n",
    "\n",
    "# Save the cleaned and extracted data to a new JSON file\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    json.dump(result, output_file, indent=4)\n",
    "\n",
    "print(f\"Cleaned and extracted data has been saved to {output_file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the extracted SPARQL queries on Wikidata\n",
    "Results for **mistral-large-instruct**.\n",
    "\n",
    "Need to build something to let them run on the local endpoint for dbpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "input_file_path = \"mistral_wiki_trans_sparql_queries.json\"\n",
    "output_file_path = \"wikidata_query_results_mistral.json\"\n",
    "\n",
    "# Wikidata local endpoint\n",
    "WIKIDATA_ENDPOINT = \"http://localhost:7001\"\n",
    "\n",
    "# Define prefixes for the queries\n",
    "PREFIXES = \"\"\"\n",
    "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "PREFIX res: <http://dbpedia.org/resource/>\n",
    "PREFIX yago: <http://dbpedia.org/class/yago/>\n",
    "PREFIX onto: <http://dbpedia.org/ontology/>\n",
    "PREFIX dbp: <http://dbpedia.org/property/>\n",
    "PREFIX dbr: <http://dbpedia.org/resource/>\n",
    "PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "PREFIX dbc: <http://dbpedia.org/resource/Category:>\n",
    "PREFIX dct: <http://purl.org/dc/terms/>\n",
    "\"\"\"\n",
    "\n",
    "# Function to check and prepend prefixes if not present\n",
    "def ensure_prefixes(query):\n",
    "    if not query.strip().startswith(\"PREFIX\"):\n",
    "        return PREFIXES + query\n",
    "    return query\n",
    "\n",
    "# Function to query the SPARQL endpoint\n",
    "def query_sparql(endpoint, query):\n",
    "    sparql = SPARQLWrapper(endpoint)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    sparql.setMethod('GET')\n",
    "    sparql.setTimeout(60)\n",
    "    \n",
    "    try:\n",
    "        return sparql.query().convert()  # Return results\n",
    "    except Exception as e:\n",
    "        print(f\"Query failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract the answers from the SPARQL query results\n",
    "def extract_answer(results):\n",
    "    if not results:\n",
    "        return [\"Query failed\"]\n",
    "    \n",
    "    if 'boolean' in results:\n",
    "        return [\"True\"] if results['boolean'] else [\"False\"]\n",
    "\n",
    "    answers = []\n",
    "    bindings = results.get('results', {}).get('bindings', [])\n",
    "    for binding in bindings:\n",
    "        for var_name in binding:\n",
    "            value = binding[var_name]['value']\n",
    "            answers.append(value)  # Append the value directly\n",
    "    return answers if answers else [\"No answer\"]\n",
    "\n",
    "# Load the input dataset\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize results list\n",
    "query_results = []\n",
    "\n",
    "# Process each query in the dataset\n",
    "for entry in data:\n",
    "    question = entry.get(\"natural_language_question\", \"\")\n",
    "    sparql_query = entry.get(\"sparql_query_kg2\", \"\")\n",
    "    \n",
    "    if not sparql_query:\n",
    "        print(f\"No SPARQL query found for question: {question}\")\n",
    "        continue\n",
    "\n",
    "    # Ensure prefixes are included in the query\n",
    "    sparql_query_with_prefixes = ensure_prefixes(sparql_query)\n",
    "\n",
    "    # Execute the query on the Wikidata endpoint\n",
    "    results = query_sparql(WIKIDATA_ENDPOINT, sparql_query_with_prefixes)\n",
    "    extracted_answers = extract_answer(results)\n",
    "    \n",
    "    # Append the query results\n",
    "    query_results.append({\n",
    "        \"natural_language_question\": question,\n",
    "        \"sparql_query\": sparql_query_with_prefixes,\n",
    "        \"answers\": extracted_answers\n",
    "    })\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    json.dump(query_results, output_file, indent=4)\n",
    "\n",
    "print(f\"Query results saved to {output_file_path}.\")\n",
    "\n",
    "# Calculate accuracy and categorize results\n",
    "total_queries = len(data)\n",
    "successful_queries = sum(1 for result in query_results if \"No answer\" not in result[\"answers\"] and \"Query failed\" not in result[\"answers\"])\n",
    "failed_queries = sum(1 for result in query_results if \"Query failed\" in result[\"answers\"])\n",
    "no_answer_queries = sum(1 for result in query_results if \"No answer\" in result[\"answers\"])\n",
    "accuracy = (successful_queries / total_queries) * 100 if total_queries > 0 else 0\n",
    "\n",
    "print(f\"Total Queries: {total_queries}\")\n",
    "print(f\"Successful Queries: {successful_queries}\")\n",
    "print(f\"Failed Queries: {failed_queries}\")\n",
    "print(f\"No Answer Queries: {no_answer_queries}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Visualize results with a bar chart\n",
    "categories = ['Successful Queries', 'Failed Queries', 'No Answer Queries']\n",
    "counts = [successful_queries, failed_queries, no_answer_queries]\n",
    "colors = ['green', 'red', 'orange']\n",
    "\n",
    "plt.bar(categories, counts, color=colors)\n",
    "plt.title('Query Results Analysis')\n",
    "plt.ylabel('Number of Queries')\n",
    "plt.xlabel('Query Outcome')\n",
    "for i, count in enumerate(counts):\n",
    "    plt.text(i, count + 0.5, str(count), ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for **lama-3.1-8b-instruct**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "input_file_path = \"lama_wiki_trans_sparql_queries.json\"\n",
    "output_file_path = \"wikidata_query_results_lama.json\"\n",
    "\n",
    "# Wikidata local endpoint\n",
    "WIKIDATA_ENDPOINT = \"http://localhost:7001\"\n",
    "\n",
    "# Define prefixes for the queries\n",
    "PREFIXES = \"\"\"\n",
    "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "PREFIX res: <http://dbpedia.org/resource/>\n",
    "PREFIX yago: <http://dbpedia.org/class/yago/>\n",
    "PREFIX onto: <http://dbpedia.org/ontology/>\n",
    "PREFIX dbp: <http://dbpedia.org/property/>\n",
    "PREFIX dbr: <http://dbpedia.org/resource/>\n",
    "PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "PREFIX dbc: <http://dbpedia.org/resource/Category:>\n",
    "PREFIX dct: <http://purl.org/dc/terms/>\n",
    "\"\"\"\n",
    "\n",
    "# Function to check and prepend prefixes if not present\n",
    "def ensure_prefixes(query):\n",
    "    if not query.strip().startswith(\"PREFIX\"):\n",
    "        return PREFIXES + query\n",
    "    return query\n",
    "\n",
    "# Function to query the SPARQL endpoint\n",
    "def query_sparql(endpoint, query):\n",
    "    sparql = SPARQLWrapper(endpoint)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    sparql.setMethod('GET')\n",
    "    sparql.setTimeout(60)\n",
    "    \n",
    "    try:\n",
    "        return sparql.query().convert()  # Return results\n",
    "    except Exception as e:\n",
    "        print(f\"Query failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract the answers from the SPARQL query results\n",
    "def extract_answer(results):\n",
    "    if not results:\n",
    "        return [\"Query failed\"]\n",
    "    \n",
    "    if 'boolean' in results:\n",
    "        return [\"True\"] if results['boolean'] else [\"False\"]\n",
    "\n",
    "    answers = []\n",
    "    bindings = results.get('results', {}).get('bindings', [])\n",
    "    for binding in bindings:\n",
    "        for var_name in binding:\n",
    "            value = binding[var_name]['value']\n",
    "            answers.append(value)  # Append the value directly\n",
    "    return answers if answers else [\"No answer\"]\n",
    "\n",
    "# Load the input dataset\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize results list\n",
    "query_results = []\n",
    "\n",
    "# Process each query in the dataset\n",
    "for entry in data:\n",
    "    question = entry.get(\"natural_language_question\", \"\")\n",
    "    sparql_query = entry.get(\"sparql_query\", \"\")\n",
    "    \n",
    "    if not sparql_query:\n",
    "        print(f\"No SPARQL query found for question: {question}\")\n",
    "        continue\n",
    "\n",
    "    # Ensure prefixes are included in the query\n",
    "    sparql_query_with_prefixes = ensure_prefixes(sparql_query)\n",
    "\n",
    "    # Execute the query on the Wikidata endpoint\n",
    "    results = query_sparql(WIKIDATA_ENDPOINT, sparql_query_with_prefixes)\n",
    "    extracted_answers = extract_answer(results)\n",
    "    \n",
    "    # Append the query results\n",
    "    query_results.append({\n",
    "        \"natural_language_question\": question,\n",
    "        \"sparql_query\": sparql_query_with_prefixes,\n",
    "        \"answers\": extracted_answers\n",
    "    })\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    json.dump(query_results, output_file, indent=4)\n",
    "\n",
    "print(f\"Query results saved to {output_file_path}.\")\n",
    "\n",
    "# Calculate accuracy and categorize results\n",
    "total_queries = len(data)\n",
    "successful_queries = sum(1 for result in query_results if \"No answer\" not in result[\"answers\"] and \"Query failed\" not in result[\"answers\"])\n",
    "failed_queries = sum(1 for result in query_results if \"Query failed\" in result[\"answers\"])\n",
    "no_answer_queries = sum(1 for result in query_results if \"No answer\" in result[\"answers\"])\n",
    "accuracy = (successful_queries / total_queries) * 100 if total_queries > 0 else 0\n",
    "\n",
    "print(f\"Total Queries: {total_queries}\")\n",
    "print(f\"Successful Queries: {successful_queries}\")\n",
    "print(f\"Failed Queries: {failed_queries}\")\n",
    "print(f\"No Answer Queries: {no_answer_queries}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Visualize results with a bar chart\n",
    "categories = ['Successful Queries', 'Failed Queries', 'No Answer Queries']\n",
    "counts = [successful_queries, failed_queries, no_answer_queries]\n",
    "colors = ['green', 'red', 'orange']\n",
    "\n",
    "plt.bar(categories, counts, color=colors)\n",
    "plt.title('Query Results Analysis')\n",
    "plt.ylabel('Number of Queries')\n",
    "plt.xlabel('Query Outcome')\n",
    "for i, count in enumerate(counts):\n",
    "    plt.text(i, count + 0.5, str(count), ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
