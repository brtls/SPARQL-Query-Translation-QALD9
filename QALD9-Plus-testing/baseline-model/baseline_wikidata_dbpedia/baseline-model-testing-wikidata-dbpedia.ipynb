{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating Wikidata queries to dbpedia with LLMs **Baseline model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary libraries\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from 100_complete_entries.json\n",
    "with open(\"../../../data/100_complete_entries.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create input dataset for the LLM\n",
    "llm_inputs = []\n",
    "\n",
    "for entry in data:\n",
    "    # Extract required fields\n",
    "    question = entry.get(\"question\", \"\")  # Natural language question\n",
    "    wikidata_query = entry.get(\"wikidata_query\", \"\")  # SPARQL query for wikidata\n",
    "    \n",
    "    # Extract entities and relations (ER2) in dbpedia\n",
    "    er2 = [\n",
    "        {\n",
    "            \"dbpedia_id\": er[\"dbpedia_id\"],\n",
    "            \"wikidata_ids\": er[\"wikidata_ids\"]\n",
    "        }\n",
    "        for er in entry.get(\"mapped_entities_relations\", {}).get(\"entities_relations\", [])\n",
    "        if er[\"wikidata_ids\"]  # Only include non-empty dbpedia mappings\n",
    "    ]\n",
    "    \n",
    "    # Skip if there are no valid dbpedia mappings\n",
    "    if not er2:\n",
    "        continue\n",
    "\n",
    "    # Construct the input for the LLM\n",
    "    llm_input = {\n",
    "        \"context\": {\n",
    "            \"natural_language_question\": question,\n",
    "            \"sparql_query_kg1\": wikidata_query,\n",
    "            \"kg1_name\": \"Wikidata\",\n",
    "            \"kg2_name\": \"DBpedia\",\n",
    "            \"er2\": er2,\n",
    "            \"instruction\": \"Given the information above, produce a SPARQL query for KG2. In your answer please hightlight the final, complete SPARQL query within the tags '<sparql>' and '</sparql>'.\"\n",
    "        }\n",
    "    }\n",
    "    llm_inputs.append(llm_input)\n",
    "\n",
    "# Save the processed dataset to a new JSON file\n",
    "with open(\"llm_input_dataset_wikidata.json\", \"w\") as file:\n",
    "    json.dump(llm_inputs, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing meta-llama-3.1-8b-instruct with 100 queries\n",
    "The model includes 8billion parameters and is one of the samallest available models at Academic Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated SPARQL queries for all questions saved to 'baseline_translated_llm_output_llama_wikidata_dbpedia.json'.\n"
     ]
    }
   ],
   "source": [
    "# Set up the LLM API connection\n",
    "api_key = '###'\n",
    "base_url = \"https://chat-ai.academiccloud.de/v1\"\n",
    "model = \"meta-llama-3.1-8b-instruct\"  # Replace with the appropriate model\n",
    "\n",
    "# Start OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "# Load the input dataset\n",
    "with open(\"llm_input_dataset_wikidata.json\", \"r\") as file:  # Use the baseline input dataset\n",
    "    llm_input_data = json.load(file)\n",
    "\n",
    "# Initialize the list to store the responses\n",
    "translated_dataset = []\n",
    "\n",
    "# Query the LLM for each entry in the dataset\n",
    "for entry in llm_input_data:\n",
    "    context = entry[\"context\"]\n",
    "    \n",
    "    # Create the prompt for the LLM\n",
    "    prompt = (\n",
    "        f\"Context:\\n\"\n",
    "        f\"Natural Language Question: {context['natural_language_question']}\\n\"\n",
    "        f\"SPARQL Query for KG1 ({context['kg1_name']}):\\n\"\n",
    "        f\"{context['sparql_query_kg1']}\\n\"\n",
    "        f\"Knowledge Graph 1 Name: {context['kg1_name']}\\n\"\n",
    "        f\"Knowledge Graph 2 Name: {context['kg2_name']}\\n\"\n",
    "        f\"Instruction: {context['instruction']}\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Query the LLM\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # Extract the generated SPARQL query from the response\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Append the response to the translated dataset\n",
    "        translated_entry = {\n",
    "            \"context\": context,\n",
    "            \"sparql_query_kg2\": response_text  # Add the translated SPARQL query\n",
    "        }\n",
    "        translated_dataset.append(translated_entry)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying LLM for question: {context.get('natural_language_question', 'unknown')}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save the translated dataset to a new JSON file\n",
    "with open(\"baseline_translated_llm_output_llama_wikidata_dbpedia.json\", \"w\") as file:\n",
    "    json.dump(translated_dataset, file, indent=4)\n",
    "\n",
    "print(\"Translated SPARQL queries for all questions saved to 'baseline_translated_llm_output_llama_wikidata_dbpedia.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Mistral-Large-instruct with 100 queries\n",
    "The model includes 123billion parameters and is the largest available model at Academic Cloud https://chat-ai.academiccloud.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated SPARQL queries for all questions saved to 'baseline_translated_llm_output_mistral_wikidata_dbpedia.json'.\n"
     ]
    }
   ],
   "source": [
    "# Set up the LLM API connection\n",
    "api_key = '###'\n",
    "base_url = \"https://chat-ai.academiccloud.de/v1\"\n",
    "model = \"mistral-large-instruct\"  # Replace with the appropriate model\n",
    "\n",
    "# Start OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "# Load the input dataset\n",
    "input_file_path = \"llm_input_dataset_wikidata.json\"  # Adjust path if needed\n",
    "output_file_path = \"baseline_translated_llm_output_mistral_wikidata_dbpedia.json\"\n",
    "\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    llm_input_data = json.load(file)\n",
    "\n",
    "# Initialize the list to store the responses\n",
    "translated_dataset = []\n",
    "\n",
    "# Query the LLM for each entry in the dataset\n",
    "for entry in llm_input_data:\n",
    "    context = entry[\"context\"]\n",
    "    \n",
    "    # Create the prompt for the LLM\n",
    "    prompt = (\n",
    "        f\"Context:\\n\"\n",
    "        f\"Natural Language Question: {context['natural_language_question']}\\n\"\n",
    "        f\"SPARQL Query for KG1 ({context['kg1_name']}):\\n\"\n",
    "        f\"{context['sparql_query_kg1']}\\n\"\n",
    "        f\"Knowledge Graph 1 Name: {context['kg1_name']}\\n\"\n",
    "        f\"Knowledge Graph 2 Name: {context['kg2_name']}\\n\"\n",
    "        f\"Instruction: {context['instruction']}\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Query the LLM\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # Extract the generated SPARQL query from the response\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Append the response to the translated dataset\n",
    "        translated_entry = {\n",
    "            \"context\": {\n",
    "                \"natural_language_question\": context[\"natural_language_question\"],\n",
    "                \"sparql_query_kg1\": context[\"sparql_query_kg1\"],\n",
    "                \"kg1_name\": context[\"kg1_name\"],\n",
    "                \"kg2_name\": context[\"kg2_name\"],\n",
    "                \"instruction\": context[\"instruction\"]\n",
    "            },\n",
    "            \"sparql_query_kg2\": response_text  # Add the translated SPARQL query\n",
    "        }\n",
    "        translated_dataset.append(translated_entry)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying LLM for question: {context.get('natural_language_question', 'unknown')}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save the translated dataset to a new JSON file\n",
    "with open(output_file_path, \"w\") as file:\n",
    "    json.dump(translated_dataset, file, indent=4)\n",
    "\n",
    "print(f\"Translated SPARQL queries for all questions saved to '{output_file_path}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
